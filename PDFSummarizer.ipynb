{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sudarshan-koirala/youtube-stuffs/blob/main/PDFSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTdPFcqYlvTz"
   },
   "source": [
    "# PDF Summarizer with few lines of code using Gradio, OpenAI and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wL3laYImIY-"
   },
   "source": [
    "## Install necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeTmdpqYqviZ"
   },
   "source": [
    "[Langchain website link](https://docs.langchain.com/docs/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "qplMQ5wNqtnG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (2.5.1+cpu)\n",
      "Requirement already satisfied: torchvision in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (0.20.1+cpu)\n",
      "Requirement already satisfied: torchaudio in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (2.5.1+cpu)\n",
      "Requirement already satisfied: filelock in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\cortech\\learning\\youtube-stuffs\\env\\lib\\site-packages (from jinja2->torch) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "_EkR21whlkn6"
   },
   "outputs": [],
   "source": [
    "!pip install -q gradio openai pypdf tiktoken langchain transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1w19ZNKxr37l"
   },
   "outputs": [],
   "source": [
    "#with open('env_vars.json', 'r') as f:\n",
    "#    env_vars = json.load(f)\n",
    "#openai.api_key = env_vars[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "vY7HQRdMpmAO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "eVPEZ7bt2vGR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83, 1609, 5963, 374, 2294, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    print(encoding.encode(string))\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "gMgR1njymD8d"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain import OpenAI, PromptTemplate, HuggingFaceHub\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "# llm = OpenAI(temperature=0)\n",
    "hub_llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\":0.9, \"max_length\":64})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHEA4UF_qix4"
   },
   "outputs": [],
   "source": [
    "#PyPDFLoader??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDE4Dm99qdYk"
   },
   "source": [
    "## LangChain part \n",
    "#### Function that takes PDF file as input and returns the summary of that PDF\n",
    "- langchain `PyPDFLoader` helps load the PDF\n",
    "- After that we can split the document in smaller chunks\n",
    "- We then use the `load_summarize_chain` to create a summarization chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "RZX-spcApXXm"
   },
   "outputs": [],
   "source": [
    "def summarize_pdf(pdf_file_path):\n",
    "    loader = PyPDFLoader(pdf_file_path)\n",
    "    docs = loader.load_and_split()\n",
    "    chain = load_summarize_chain(hub_llm, chain_type=\"map_reduce\")\n",
    "    summary = chain.run(docs)   \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "4N0oxt_9tGgQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The GPT4All ecosystem includes a number of language models that are available for use in the GPT4All-Snoozy project.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize = summarize_pdf(\"data/gpt4all.pdf\")\n",
    "summarize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "MCO5KwFRxd7Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'data/gpt4all.pdf', 'page': 0}, page_content='GPT4All: An Ecosystem of Open Source Compressed Language Models\\nYuvanesh Anand\\nNomic AI\\nyuvanesh@nomic.ai\\nZach Nussbaum\\nNomic AI\\nzach@nomic.ai\\nAdam Treat\\nNomic AI\\nadam@nomic.ai\\nAaron Miller\\nNomic AI\\naaron@nomic.ai\\nRichard Guo\\nNomic AI\\nrichard@nomic.ai\\nBen Schmidt\\nNomic AI\\nben@nomic.ai\\nGPT4All Community\\nPlanet Earth\\nBrandon Duderstadt∗\\nNomic AI\\nbrandon@nomic.ai\\nAndriy Mulyar∗\\nNomic AI\\nandriy@nomic.ai\\nAbstract\\nLarge language models (LLMs) have recently\\nachieved human-level performance on a range\\nof professional and academic benchmarks. The\\naccessibility of these models has lagged behind\\ntheir performance. State-of-the-art LLMs re-\\nquire costly infrastructure; are only accessible\\nvia rate-limited, geo-locked, and censored web\\ninterfaces; and lack publicly available code and\\ntechnical reports.\\nIn this paper, we tell the story of GPT4All, a\\npopular open source repository that aims to\\ndemocratize access to LLMs. We outline the\\ntechnical details of the original GPT4All model\\nfamily, as well as the evolution of the GPT4All\\nproject from a single model into a fully fledged\\nopen source ecosystem. It is our hope that\\nthis paper acts as both a technical overview of\\nthe original GPT4All models as well as a case\\nstudy on the subsequent growth of the GPT4All\\nopen source ecosystem.\\n1 Introduction\\nOn March 14 2023, OpenAI released GPT-4, a large\\nlanguage model capable of achieving human level per-\\nformance on a variety of professional and academic\\nbenchmarks. Despite the popularity of the release,\\nthe GPT-4 technical report (OpenAI, 2023) contained\\nvirtually no details regarding the architecture, hard-\\nware, training compute, dataset construction, or training\\nmethod used to create the model. Moreover, users could\\nonly access the model through the internet interface at\\nchat.openai.com, which was severely rate limited and\\nunavailable in several locales (e.g. Italy) (BBC News,\\n2023). Additionally, GPT-4 refused to answer a wide\\n∗ Shared Senior Authorship\\nvariety of queries, responding only with the now infa-\\nmous \"As an AI Language Model, I cannot...\" prefix\\n(Vincent, 2023). These transparency and accessibility\\nconcerns spurred several developers to begin creating\\nopen source large language model (LLM) alternatives.\\nSeveral grassroots efforts focused on fine tuning Meta’s\\nopen code LLaMA model (Touvron et al., 2023; McMil-\\nlan, 2023), whose weights were leaked on BitTorrent\\nless than a week prior to the release of GPT-4 (Verge,\\n2023). GPT4All started as one of these variants.\\nIn this paper, we tell the story of GPT4All. We com-\\nment on the technical details of the original GPT4All\\nmodel (Anand et al., 2023), as well as the evolution of\\nGPT4All from a single model to an ecosystem of several\\nmodels. We remark on the impact that the project has\\nhad on the open source community, and discuss future\\ndirections. It is our hope that this paper acts as both a\\ntechnical overview of the original GPT4All models as\\nwell as a case study on the subsequent growth of the\\nGPT4All open source ecosystem.\\n2 The Original GPT4All Model\\n2.1 Data Collection and Curation\\nTo train the original GPT4All model, we collected\\nroughly one million prompt-response pairs using the\\nGPT-3.5-Turbo OpenAI API between March 20, 2023\\nand March 26th, 2023. In particular, we gathered GPT-\\n3.5-Turbo responses to prompts of three publicly avail-\\nable datasets: the unified chip2 subset of LAION OIG,\\na random sub-sample of Stackoverflow Questions, and\\na sub-sample of Bigscience/P3 (Sanh et al., 2021). Fol-\\nlowing the approach in Stanford Alpaca (Taori et al.,\\n2023), an open source LLaMA variant that came just be-\\nfore GPT4All, we focused substantial effort on dataset\\ncuration.\\nThe collected dataset was loaded into Atlas (AI,\\n2023)—a visual interface for exploring and tagging mas-\\nsive unstructured datasets —for data curation. Using At-\\narXiv:2311.04931v1  [cs.CL]  6 Nov 2023')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just to show you how it works\n",
    "loader = PyPDFLoader('data/gpt4all.pdf')\n",
    "doc=loader.load_and_split()\n",
    "print(len(doc))\n",
    "doc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXEcdNg5rKDi"
   },
   "source": [
    "## Create a simple gradio UI (if you prefer UI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oth5CY7BrFH4"
   },
   "outputs": [],
   "source": [
    "\n",
    "input_pdf_path = gr.components.Textbox(label=\"Provide the PDF file path\")\n",
    "output_summary = gr.components.Textbox(label=\"Summary\")\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=summarize_pdf,\n",
    "    inputs=input_pdf_path,\n",
    "    outputs=output_summary,\n",
    "    title=\"PDF Summarizer\",\n",
    "    description=\"Provide PDF file path to get the summary.\",\n",
    ").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "678_yAVTrjg2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPUfLVhYfgtLK49w9kl4hbn",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
